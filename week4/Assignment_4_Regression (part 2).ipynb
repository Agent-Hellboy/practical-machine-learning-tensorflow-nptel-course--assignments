{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment 4 - Regression.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLVhCarMwG70",
        "colab_type": "text"
      },
      "source": [
        "### **Setup**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "teXJ1XpSwdvR",
        "colab_type": "text"
      },
      "source": [
        "Install and import all the necessary libraries for the assignment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQNMrFD-ZBwK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        },
        "outputId": "574ea438-c4b3-466b-b890-c7f664eeaea3"
      },
      "source": [
        "!pip install tensorflow==2.0.0-rc0\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_boston\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "\n",
        "tf.random.set_seed(1)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==2.0.0-rc0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fb/4b/77f0965ec7e8a76d3dcd6a22ca8bbd2b934cd92c4ded43fef6bea5ff3258/tensorflow-2.0.0rc0-cp36-cp36m-manylinux2010_x86_64.whl (86.3MB)\n",
            "\u001b[K     |████████████████████████████████| 86.3MB 1.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-rc0) (1.11.2)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-rc0) (1.15.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-rc0) (0.8.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-rc0) (0.33.6)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-rc0) (1.16.5)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-rc0) (3.7.1)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-rc0) (0.3.2)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-rc0) (1.0.8)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-rc0) (0.8.0)\n",
            "Collecting tb-nightly<1.15.0a20190807,>=1.15.0a20190806 (from tensorflow==2.0.0-rc0)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bc/88/24b5fb7280e74c7cf65bde47c171547fd02afb3840cff41bcbe9270650f5/tb_nightly-1.15.0a20190806-py3-none-any.whl (4.3MB)\n",
            "\u001b[K     |████████████████████████████████| 4.3MB 42.5MB/s \n",
            "\u001b[?25hCollecting tf-estimator-nightly<1.14.0.dev2019080602,>=1.14.0.dev2019080601 (from tensorflow==2.0.0-rc0)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/21/28/f2a27a62943d5f041e4a6fd404b2d21cb7c59b2242a4e73b03d9ba166552/tf_estimator_nightly-1.14.0.dev2019080601-py2.py3-none-any.whl (501kB)\n",
            "\u001b[K     |████████████████████████████████| 501kB 55.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-rc0) (1.1.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-rc0) (3.0.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-rc0) (1.12.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-rc0) (0.1.7)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-rc0) (1.1.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==2.0.0-rc0) (41.2.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow==2.0.0-rc0) (2.8.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.15.0a20190807,>=1.15.0a20190806->tensorflow==2.0.0-rc0) (3.1.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.15.0a20190807,>=1.15.0a20190806->tensorflow==2.0.0-rc0) (0.16.0)\n",
            "Installing collected packages: tb-nightly, tf-estimator-nightly, tensorflow\n",
            "  Found existing installation: tensorflow 1.14.0\n",
            "    Uninstalling tensorflow-1.14.0:\n",
            "      Successfully uninstalled tensorflow-1.14.0\n",
            "Successfully installed tb-nightly-1.15.0a20190806 tensorflow-2.0.0rc0 tf-estimator-nightly-1.14.0.dev2019080601\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGGgAUOKwsWA",
        "colab_type": "text"
      },
      "source": [
        "### **Importing the dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GOe2azQOdmND",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "boston_dataset = load_boston()\n",
        "\n",
        "data_X = pd.DataFrame(boston_dataset.data, columns=boston_dataset.feature_names)\n",
        "data_Y = pd.DataFrame(boston_dataset.target, columns=[\"target\"])\n",
        "data = pd.concat([data_X, data_Y], axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9gD5esSxfxjs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "9e0fed42-966e-47b7-b024-106346e118aa"
      },
      "source": [
        "train, test = train_test_split(data, test_size=0.2, random_state=1)\n",
        "train, val = train_test_split(train, test_size=0.2, random_state=1)\n",
        "print(len(train), \"train examples\")\n",
        "print(len(val), \"validation examples\")\n",
        "print(len(test), \"test examples\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "323 train examples\n",
            "81 validation examples\n",
            "102 test examples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ZTeC55HxDeT",
        "colab_type": "text"
      },
      "source": [
        "Converting the Pandas DataFrames into Tensorflow Datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nF4GRPPLdTIe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def df_to_dataset(dataframe, shuffle=True, batch_size=32):\n",
        "  dataframe = dataframe.copy()\n",
        "  labels = dataframe.pop('target')\n",
        "  ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n",
        "  if shuffle:\n",
        "    ds = ds.shuffle(buffer_size=len(dataframe))\n",
        "  ds = ds.batch(batch_size)\n",
        "  return ds"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UdZy7p3AaTRT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "b3000b26-38e3-4d83-8c02-4b09d548c899"
      },
      "source": [
        "batch_size = 32\n",
        "train_ds = df_to_dataset(train, True, batch_size)\n",
        "val_ds = df_to_dataset(val, False, batch_size)\n",
        "test_ds = df_to_dataset(test, False, batch_size)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/data/util/random_seed.py:58: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63KuTr4sxMl6",
        "colab_type": "text"
      },
      "source": [
        "### Defining Feature Columns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "380jHjPokFUy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "563c42b4-4bdf-443f-9327-5b923368ec58"
      },
      "source": [
        "for feature_batch,label_batch in train_ds.take(1):\n",
        "  print(list(feature_batch.keys()))\n",
        "  \n",
        "  \n",
        "feature_columns = []\n",
        "for header in ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT']:\n",
        "  feature_columns.append(tf.feature_column.numeric_column(header))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykVCMrdMxVB5",
        "colab_type": "text"
      },
      "source": [
        "### Building the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oAc9LpVzqql9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "feature_layer = tf.keras.layers.DenseFeatures(feature_columns)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6B9FgRyyGXe",
        "colab_type": "text"
      },
      "source": [
        "Model should contain following layers:\n",
        "\n",
        "```\n",
        "feature_layer\n",
        "\n",
        "Dense(1, activation=None)\n",
        "```\n",
        "\n",
        "Use 'Adam' optimizer\n",
        "\n",
        "Use 'mse' as your loss and metric"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZInuZ8D0xsu1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Build and compile your model in this cell.\n",
        "model=tf.keras.Sequential([\n",
        "    feature_layer,\n",
        "    tf.keras.layers.Dense(4,activation='sigmoid'),\n",
        "    tf.keras.layers.Dense(1,activation=None),\n",
        "    \n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='mse',\n",
        "    metrics=['mse']\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Igdzl3wasRo6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "de0fe5e3-d2e0-4187-b6d4-a63a621fc073"
      },
      "source": [
        "model.fit(train_ds, validation_data=val_ds, epochs=200)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer sequential_1 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
            "\n",
            "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
            "\n",
            "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
            "\n",
            "Epoch 1/200\n",
            "WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7f2a6e010598> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7f2a6e010598> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "11/11 [==============================] - 1s 63ms/step - loss: 608.4514 - mse: 622.3061 - val_loss: 0.0000e+00 - val_mse: 0.0000e+00\n",
            "Epoch 2/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 617.9256 - mse: 621.0661 - val_loss: 587.4972 - val_mse: 583.6238\n",
            "Epoch 3/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 619.4310 - mse: 619.8081 - val_loss: 586.1846 - val_mse: 582.2954\n",
            "Epoch 4/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 617.6392 - mse: 618.4169 - val_loss: 584.8930 - val_mse: 580.9915\n",
            "Epoch 5/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 618.8815 - mse: 616.8040 - val_loss: 583.2503 - val_mse: 579.2725\n",
            "Epoch 6/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 616.1199 - mse: 614.7336 - val_loss: 580.4500 - val_mse: 576.3249\n",
            "Epoch 7/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 613.9388 - mse: 611.5961 - val_loss: 576.3924 - val_mse: 572.5321\n",
            "Epoch 8/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 606.3600 - mse: 607.9877 - val_loss: 571.3881 - val_mse: 567.7949\n",
            "Epoch 9/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 598.8895 - mse: 604.2186 - val_loss: 569.0296 - val_mse: 565.3956\n",
            "Epoch 10/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 601.8963 - mse: 600.9982 - val_loss: 563.5234 - val_mse: 559.9270\n",
            "Epoch 11/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 597.4506 - mse: 595.7445 - val_loss: 556.9941 - val_mse: 553.4641\n",
            "Epoch 12/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 588.9067 - mse: 589.0875 - val_loss: 551.2829 - val_mse: 548.1782\n",
            "Epoch 13/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 579.3119 - mse: 582.4333 - val_loss: 547.8217 - val_mse: 544.5947\n",
            "Epoch 14/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 579.2951 - mse: 579.2884 - val_loss: 544.8205 - val_mse: 541.9200\n",
            "Epoch 15/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 578.6834 - mse: 577.0652 - val_loss: 542.6772 - val_mse: 539.8185\n",
            "Epoch 16/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 575.7486 - mse: 574.9354 - val_loss: 540.6154 - val_mse: 537.7887\n",
            "Epoch 17/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 573.2685 - mse: 572.8384 - val_loss: 538.5780 - val_mse: 535.7822\n",
            "Epoch 18/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 571.4410 - mse: 570.6810 - val_loss: 536.5565 - val_mse: 533.7839\n",
            "Epoch 19/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 569.2048 - mse: 567.8285 - val_loss: 532.8498 - val_mse: 529.7818\n",
            "Epoch 20/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 564.5784 - mse: 564.2786 - val_loss: 530.4134 - val_mse: 527.2842\n",
            "Epoch 21/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 564.1032 - mse: 561.0068 - val_loss: 526.1389 - val_mse: 522.9719\n",
            "Epoch 22/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 559.3999 - mse: 557.6683 - val_loss: 521.7196 - val_mse: 518.8868\n",
            "Epoch 23/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 557.2382 - mse: 555.4102 - val_loss: 519.1106 - val_mse: 516.2715\n",
            "Epoch 24/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 548.9356 - mse: 553.3102 - val_loss: 516.8328 - val_mse: 513.9684\n",
            "Epoch 25/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 548.7279 - mse: 551.1536 - val_loss: 514.7971 - val_mse: 511.9482\n",
            "Epoch 26/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 549.3908 - mse: 549.0698 - val_loss: 512.7912 - val_mse: 509.9647\n",
            "Epoch 27/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 547.5478 - mse: 547.0532 - val_loss: 510.8069 - val_mse: 508.0034\n",
            "Epoch 28/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 544.8624 - mse: 545.0490 - val_loss: 508.8535 - val_mse: 506.0715\n",
            "Epoch 29/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 545.1549 - mse: 543.0685 - val_loss: 506.9321 - val_mse: 504.1704\n",
            "Epoch 30/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 543.2878 - mse: 541.1489 - val_loss: 505.0500 - val_mse: 502.3081\n",
            "Epoch 31/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 541.7999 - mse: 539.2824 - val_loss: 503.1715 - val_mse: 500.4505\n",
            "Epoch 32/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 540.3204 - mse: 537.4266 - val_loss: 501.3289 - val_mse: 498.6280\n",
            "Epoch 33/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 538.6536 - mse: 535.5945 - val_loss: 499.5257 - val_mse: 496.8437\n",
            "Epoch 34/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 528.6099 - mse: 533.7913 - val_loss: 497.7101 - val_mse: 495.0483\n",
            "Epoch 35/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 528.5069 - mse: 531.9471 - val_loss: 495.8732 - val_mse: 493.2320\n",
            "Epoch 36/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 530.0358 - mse: 530.0815 - val_loss: 494.0168 - val_mse: 491.3964\n",
            "Epoch 37/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 529.2809 - mse: 528.2578 - val_loss: 492.1650 - val_mse: 489.5656\n",
            "Epoch 38/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 527.8125 - mse: 526.4078 - val_loss: 490.3636 - val_mse: 487.7841\n",
            "Epoch 39/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 524.9080 - mse: 524.6011 - val_loss: 488.5739 - val_mse: 486.0141\n",
            "Epoch 40/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 524.0702 - mse: 522.8044 - val_loss: 486.7681 - val_mse: 484.2285\n",
            "Epoch 41/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 518.0548 - mse: 521.0089 - val_loss: 484.9689 - val_mse: 482.4496\n",
            "Epoch 42/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 515.5160 - mse: 519.1980 - val_loss: 483.1467 - val_mse: 480.6479\n",
            "Epoch 43/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 516.5215 - mse: 517.3751 - val_loss: 481.3363 - val_mse: 478.8576\n",
            "Epoch 44/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 518.0409 - mse: 515.5648 - val_loss: 479.5847 - val_mse: 477.1249\n",
            "Epoch 45/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 513.6948 - mse: 513.8301 - val_loss: 477.8639 - val_mse: 475.4230\n",
            "Epoch 46/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 512.2596 - mse: 512.0931 - val_loss: 476.1110 - val_mse: 473.6897\n",
            "Epoch 47/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 508.7150 - mse: 510.3362 - val_loss: 474.3797 - val_mse: 471.9767\n",
            "Epoch 48/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 509.5637 - mse: 508.5826 - val_loss: 472.6325 - val_mse: 470.2487\n",
            "Epoch 49/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 506.7417 - mse: 506.8520 - val_loss: 470.9055 - val_mse: 468.5413\n",
            "Epoch 50/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 507.1166 - mse: 505.1190 - val_loss: 469.1773 - val_mse: 466.8325\n",
            "Epoch 51/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 502.4266 - mse: 503.4050 - val_loss: 467.4732 - val_mse: 465.1474\n",
            "Epoch 52/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 500.3876 - mse: 501.6762 - val_loss: 465.7759 - val_mse: 463.4683\n",
            "Epoch 53/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 501.6390 - mse: 499.9739 - val_loss: 464.0869 - val_mse: 461.7975\n",
            "Epoch 54/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 500.1721 - mse: 498.2908 - val_loss: 462.4128 - val_mse: 460.1412\n",
            "Epoch 55/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 497.8878 - mse: 496.6107 - val_loss: 460.7704 - val_mse: 458.5165\n",
            "Epoch 56/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 495.9740 - mse: 494.9636 - val_loss: 459.1163 - val_mse: 456.8810\n",
            "Epoch 57/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 493.1646 - mse: 493.3093 - val_loss: 457.4584 - val_mse: 455.2418\n",
            "Epoch 58/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 490.2528 - mse: 491.6242 - val_loss: 455.7963 - val_mse: 453.5981\n",
            "Epoch 59/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 492.8280 - mse: 489.9652 - val_loss: 454.1157 - val_mse: 451.9362\n",
            "Epoch 60/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 488.6725 - mse: 488.3063 - val_loss: 452.5095 - val_mse: 450.3474\n",
            "Epoch 61/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 488.7271 - mse: 486.6827 - val_loss: 450.8801 - val_mse: 448.7359\n",
            "Epoch 62/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 486.0035 - mse: 485.0497 - val_loss: 449.2649 - val_mse: 447.1386\n",
            "Epoch 63/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 484.0299 - mse: 483.4289 - val_loss: 447.6363 - val_mse: 445.5284\n",
            "Epoch 64/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 484.5975 - mse: 481.7832 - val_loss: 446.0279 - val_mse: 443.9374\n",
            "Epoch 65/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 482.0403 - mse: 480.1878 - val_loss: 444.4403 - val_mse: 442.3672\n",
            "Epoch 66/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 479.3332 - mse: 478.5862 - val_loss: 442.8521 - val_mse: 440.7960\n",
            "Epoch 67/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 476.0610 - mse: 476.9696 - val_loss: 441.2644 - val_mse: 439.2256\n",
            "Epoch 68/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 478.1126 - mse: 475.3672 - val_loss: 439.6520 - val_mse: 437.6312\n",
            "Epoch 69/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 476.0398 - mse: 473.7843 - val_loss: 438.0725 - val_mse: 436.0693\n",
            "Epoch 70/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 471.9830 - mse: 472.1944 - val_loss: 436.5245 - val_mse: 434.5385\n",
            "Epoch 71/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 471.5771 - mse: 470.6300 - val_loss: 434.9285 - val_mse: 432.9604\n",
            "Epoch 72/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 468.1736 - mse: 469.0281 - val_loss: 433.3646 - val_mse: 431.4142\n",
            "Epoch 73/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 465.1547 - mse: 467.4387 - val_loss: 431.7724 - val_mse: 429.8398\n",
            "Epoch 74/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 468.3097 - mse: 465.8372 - val_loss: 430.1773 - val_mse: 428.2626\n",
            "Epoch 75/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 465.9740 - mse: 464.2486 - val_loss: 428.6477 - val_mse: 426.7498\n",
            "Epoch 76/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 459.1121 - mse: 462.7155 - val_loss: 427.0994 - val_mse: 425.2187\n",
            "Epoch 77/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 462.8324 - mse: 461.1324 - val_loss: 425.5332 - val_mse: 423.6701\n",
            "Epoch 78/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 459.0064 - mse: 459.5828 - val_loss: 423.9885 - val_mse: 422.1423\n",
            "Epoch 79/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 454.0806 - mse: 458.0072 - val_loss: 422.4339 - val_mse: 420.6039\n",
            "Epoch 80/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 451.2816 - mse: 456.4278 - val_loss: 420.8476 - val_mse: 419.0352\n",
            "Epoch 81/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 457.1266 - mse: 454.8205 - val_loss: 419.2650 - val_mse: 417.4707\n",
            "Epoch 82/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 454.4200 - mse: 453.2597 - val_loss: 417.7410 - val_mse: 415.9634\n",
            "Epoch 83/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 454.5814 - mse: 451.7309 - val_loss: 416.2227 - val_mse: 414.4619\n",
            "Epoch 84/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 451.1311 - mse: 450.2229 - val_loss: 414.7417 - val_mse: 412.9973\n",
            "Epoch 85/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 450.4011 - mse: 448.7289 - val_loss: 413.2422 - val_mse: 411.5149\n",
            "Epoch 86/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 447.4129 - mse: 447.2059 - val_loss: 411.7557 - val_mse: 410.0452\n",
            "Epoch 87/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 446.8849 - mse: 445.7115 - val_loss: 410.2457 - val_mse: 408.5523\n",
            "Epoch 88/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 445.3172 - mse: 444.1925 - val_loss: 408.7571 - val_mse: 407.0808\n",
            "Epoch 89/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 443.0757 - mse: 442.6894 - val_loss: 407.2764 - val_mse: 405.6168\n",
            "Epoch 90/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 442.3413 - mse: 441.1981 - val_loss: 405.7905 - val_mse: 404.1481\n",
            "Epoch 91/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 442.1973 - mse: 439.7167 - val_loss: 404.3058 - val_mse: 402.6802\n",
            "Epoch 92/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 432.1710 - mse: 438.2192 - val_loss: 402.8682 - val_mse: 401.2590\n",
            "Epoch 93/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 436.1946 - mse: 436.7450 - val_loss: 401.3629 - val_mse: 399.7706\n",
            "Epoch 94/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 431.6007 - mse: 435.2323 - val_loss: 399.8656 - val_mse: 398.2905\n",
            "Epoch 95/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 434.7857 - mse: 433.7092 - val_loss: 398.3549 - val_mse: 396.7972\n",
            "Epoch 96/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 432.4890 - mse: 432.2068 - val_loss: 396.8970 - val_mse: 395.3559\n",
            "Epoch 97/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 432.8853 - mse: 430.7398 - val_loss: 395.4615 - val_mse: 393.9364\n",
            "Epoch 98/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 427.7542 - mse: 429.3008 - val_loss: 394.0338 - val_mse: 392.5247\n",
            "Epoch 99/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 427.9004 - mse: 427.8478 - val_loss: 392.5652 - val_mse: 391.0723\n",
            "Epoch 100/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 424.6111 - mse: 426.3787 - val_loss: 391.1007 - val_mse: 389.6244\n",
            "Epoch 101/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 425.5118 - mse: 424.8957 - val_loss: 389.6276 - val_mse: 388.1684\n",
            "Epoch 102/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 423.1562 - mse: 423.4085 - val_loss: 388.1950 - val_mse: 386.7520\n",
            "Epoch 103/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 418.9704 - mse: 421.9622 - val_loss: 386.7564 - val_mse: 385.3300\n",
            "Epoch 104/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 420.2413 - mse: 420.5096 - val_loss: 385.3090 - val_mse: 383.8991\n",
            "Epoch 105/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 421.8680 - mse: 419.0543 - val_loss: 383.8839 - val_mse: 382.4907\n",
            "Epoch 106/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 415.4185 - mse: 417.6421 - val_loss: 382.4984 - val_mse: 381.1208\n",
            "Epoch 107/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 413.4766 - mse: 416.2202 - val_loss: 381.0670 - val_mse: 379.7061\n",
            "Epoch 108/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 411.8373 - mse: 414.7650 - val_loss: 379.6267 - val_mse: 378.2826\n",
            "Epoch 109/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 412.1048 - mse: 413.3015 - val_loss: 378.1811 - val_mse: 376.8539\n",
            "Epoch 110/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 413.6430 - mse: 411.8698 - val_loss: 376.7430 - val_mse: 375.4324\n",
            "Epoch 111/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 411.7167 - mse: 410.4236 - val_loss: 375.3791 - val_mse: 374.0843\n",
            "Epoch 112/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 403.4347 - mse: 409.0458 - val_loss: 373.9960 - val_mse: 372.7176\n",
            "Epoch 113/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 408.6091 - mse: 407.6401 - val_loss: 372.5853 - val_mse: 371.3232\n",
            "Epoch 114/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 407.1088 - mse: 406.2253 - val_loss: 371.2124 - val_mse: 369.9665\n",
            "Epoch 115/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 402.9782 - mse: 404.8352 - val_loss: 369.8348 - val_mse: 368.6048\n",
            "Epoch 116/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 399.8092 - mse: 403.4313 - val_loss: 368.4372 - val_mse: 367.2234\n",
            "Epoch 117/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 403.9743 - mse: 402.0057 - val_loss: 367.0417 - val_mse: 365.8441\n",
            "Epoch 118/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 402.7629 - mse: 400.6310 - val_loss: 365.6803 - val_mse: 364.4989\n",
            "Epoch 119/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 398.5298 - mse: 399.2609 - val_loss: 364.3505 - val_mse: 363.1843\n",
            "Epoch 120/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 399.8103 - mse: 397.9061 - val_loss: 362.9909 - val_mse: 361.8406\n",
            "Epoch 121/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 398.1024 - mse: 396.5472 - val_loss: 361.6612 - val_mse: 360.5266\n",
            "Epoch 122/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 395.5964 - mse: 395.2098 - val_loss: 360.3359 - val_mse: 359.2164\n",
            "Epoch 123/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 396.0227 - mse: 393.8684 - val_loss: 359.0108 - val_mse: 357.9064\n",
            "Epoch 124/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 389.0980 - mse: 392.5318 - val_loss: 357.7036 - val_mse: 356.6146\n",
            "Epoch 125/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 392.8107 - mse: 391.2079 - val_loss: 356.3764 - val_mse: 355.3023\n",
            "Epoch 126/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 391.2146 - mse: 389.8613 - val_loss: 355.0667 - val_mse: 354.0068\n",
            "Epoch 127/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 384.4374 - mse: 388.5449 - val_loss: 353.7400 - val_mse: 352.6924\n",
            "Epoch 128/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 386.6527 - mse: 387.1898 - val_loss: 352.3789 - val_mse: 351.3400\n",
            "Epoch 129/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 387.2479 - mse: 385.8648 - val_loss: 351.0051 - val_mse: 349.9775\n",
            "Epoch 130/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 384.7311 - mse: 384.5061 - val_loss: 349.5667 - val_mse: 348.5297\n",
            "Epoch 131/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 382.4441 - mse: 383.1235 - val_loss: 348.1861 - val_mse: 347.1493\n",
            "Epoch 132/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 381.4348 - mse: 381.8182 - val_loss: 346.8345 - val_mse: 345.8097\n",
            "Epoch 133/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 382.1886 - mse: 380.4811 - val_loss: 345.5073 - val_mse: 344.4963\n",
            "Epoch 134/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 381.0183 - mse: 379.1665 - val_loss: 344.2181 - val_mse: 343.2205\n",
            "Epoch 135/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 377.6290 - mse: 377.8665 - val_loss: 342.9426 - val_mse: 341.9592\n",
            "Epoch 136/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 373.5739 - mse: 376.5034 - val_loss: 341.6175 - val_mse: 340.6416\n",
            "Epoch 137/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 371.4001 - mse: 374.8148 - val_loss: 339.4791 - val_mse: 338.6666\n",
            "Epoch 138/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 371.8455 - mse: 371.1400 - val_loss: 337.8588 - val_mse: 337.0219\n",
            "Epoch 139/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 370.6572 - mse: 369.4499 - val_loss: 336.0172 - val_mse: 335.2524\n",
            "Epoch 140/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 369.4943 - mse: 367.6321 - val_loss: 333.5112 - val_mse: 332.8918\n",
            "Epoch 141/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 365.5508 - mse: 365.5381 - val_loss: 331.1723 - val_mse: 330.4564\n",
            "Epoch 142/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 363.2197 - mse: 363.2195 - val_loss: 329.0169 - val_mse: 328.1974\n",
            "Epoch 143/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 361.0872 - mse: 361.1115 - val_loss: 327.1095 - val_mse: 326.2318\n",
            "Epoch 144/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 360.1642 - mse: 358.5879 - val_loss: 323.0776 - val_mse: 322.3065\n",
            "Epoch 145/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 352.7911 - mse: 350.5698 - val_loss: 314.0007 - val_mse: 313.4404\n",
            "Epoch 146/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 345.4229 - mse: 344.3830 - val_loss: 305.9514 - val_mse: 305.7224\n",
            "Epoch 147/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 339.4862 - mse: 338.8159 - val_loss: 302.8058 - val_mse: 302.5192\n",
            "Epoch 148/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 336.9854 - mse: 335.1821 - val_loss: 300.2451 - val_mse: 299.9921\n",
            "Epoch 149/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 331.5200 - mse: 332.1404 - val_loss: 297.8501 - val_mse: 297.6287\n",
            "Epoch 150/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 330.2984 - mse: 329.6198 - val_loss: 295.5187 - val_mse: 295.3282\n",
            "Epoch 151/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 326.3425 - mse: 327.2637 - val_loss: 293.2790 - val_mse: 293.1184\n",
            "Epoch 152/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 326.4005 - mse: 324.7582 - val_loss: 291.0980 - val_mse: 290.9666\n",
            "Epoch 153/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 323.5190 - mse: 322.4201 - val_loss: 289.0764 - val_mse: 288.9721\n",
            "Epoch 154/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 320.3627 - mse: 320.3869 - val_loss: 287.0912 - val_mse: 287.0137\n",
            "Epoch 155/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 317.0772 - mse: 318.3317 - val_loss: 285.1427 - val_mse: 285.0917\n",
            "Epoch 156/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 318.4444 - mse: 316.3326 - val_loss: 283.2380 - val_mse: 283.2128\n",
            "Epoch 157/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 315.3750 - mse: 314.4182 - val_loss: 281.4184 - val_mse: 281.4182\n",
            "Epoch 158/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 309.5676 - mse: 312.5473 - val_loss: 279.6106 - val_mse: 279.6350\n",
            "Epoch 159/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 310.5172 - mse: 310.6851 - val_loss: 277.7928 - val_mse: 277.8422\n",
            "Epoch 160/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 308.1508 - mse: 308.8326 - val_loss: 276.0178 - val_mse: 276.0916\n",
            "Epoch 161/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 307.7658 - mse: 307.0067 - val_loss: 274.2746 - val_mse: 274.3724\n",
            "Epoch 162/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 302.3435 - mse: 305.2251 - val_loss: 272.5769 - val_mse: 272.6982\n",
            "Epoch 163/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 305.7729 - mse: 303.4689 - val_loss: 270.8837 - val_mse: 271.0285\n",
            "Epoch 164/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 300.3005 - mse: 301.7711 - val_loss: 269.2827 - val_mse: 269.4499\n",
            "Epoch 165/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 300.9526 - mse: 300.1066 - val_loss: 267.6215 - val_mse: 267.8119\n",
            "Epoch 166/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 296.9574 - mse: 298.3984 - val_loss: 266.0298 - val_mse: 266.2424\n",
            "Epoch 167/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 299.0075 - mse: 296.7603 - val_loss: 264.4085 - val_mse: 264.6439\n",
            "Epoch 168/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 292.9369 - mse: 295.1298 - val_loss: 262.8805 - val_mse: 263.1374\n",
            "Epoch 169/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 293.9709 - mse: 293.5334 - val_loss: 261.2882 - val_mse: 261.5677\n",
            "Epoch 170/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 292.2081 - mse: 291.9132 - val_loss: 259.7462 - val_mse: 260.0475\n",
            "Epoch 171/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 290.0575 - mse: 290.3188 - val_loss: 258.2406 - val_mse: 258.5631\n",
            "Epoch 172/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 288.7684 - mse: 288.7711 - val_loss: 256.7425 - val_mse: 257.0863\n",
            "Epoch 173/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 288.4468 - mse: 287.2397 - val_loss: 255.2486 - val_mse: 255.6137\n",
            "Epoch 174/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 285.1463 - mse: 285.7083 - val_loss: 253.7967 - val_mse: 254.1825\n",
            "Epoch 175/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 286.2587 - mse: 284.2062 - val_loss: 252.3403 - val_mse: 252.7469\n",
            "Epoch 176/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 284.1003 - mse: 282.7425 - val_loss: 250.9427 - val_mse: 251.3694\n",
            "Epoch 177/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 282.4519 - mse: 281.3016 - val_loss: 249.5712 - val_mse: 250.0177\n",
            "Epoch 178/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 280.2817 - mse: 279.8625 - val_loss: 248.2019 - val_mse: 248.6680\n",
            "Epoch 179/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 280.0454 - mse: 278.4631 - val_loss: 246.7921 - val_mse: 247.2785\n",
            "Epoch 180/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 278.5726 - mse: 277.0226 - val_loss: 245.4331 - val_mse: 245.9393\n",
            "Epoch 181/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 276.9131 - mse: 275.6348 - val_loss: 244.0880 - val_mse: 244.6136\n",
            "Epoch 182/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 272.9010 - mse: 274.2273 - val_loss: 242.7609 - val_mse: 243.3058\n",
            "Epoch 183/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 266.2114 - mse: 272.8359 - val_loss: 241.3799 - val_mse: 241.9448\n",
            "Epoch 184/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 272.6389 - mse: 271.3716 - val_loss: 239.9520 - val_mse: 240.5379\n",
            "Epoch 185/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 271.0612 - mse: 269.9687 - val_loss: 238.5901 - val_mse: 239.1960\n",
            "Epoch 186/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 267.2957 - mse: 268.5491 - val_loss: 237.2965 - val_mse: 237.9213\n",
            "Epoch 187/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 268.1752 - mse: 267.2224 - val_loss: 235.9472 - val_mse: 236.5918\n",
            "Epoch 188/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 267.2620 - mse: 265.8329 - val_loss: 234.6663 - val_mse: 235.3298\n",
            "Epoch 189/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 264.8045 - mse: 264.5256 - val_loss: 233.3970 - val_mse: 234.0792\n",
            "Epoch 190/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 262.6010 - mse: 263.2157 - val_loss: 232.1143 - val_mse: 232.8155\n",
            "Epoch 191/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 261.3088 - mse: 261.8762 - val_loss: 230.8213 - val_mse: 231.5417\n",
            "Epoch 192/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 261.8385 - mse: 260.5454 - val_loss: 229.5274 - val_mse: 230.2670\n",
            "Epoch 193/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 259.5394 - mse: 259.2193 - val_loss: 228.2862 - val_mse: 229.0443\n",
            "Epoch 194/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 259.4591 - mse: 257.9397 - val_loss: 227.0586 - val_mse: 227.8350\n",
            "Epoch 195/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 257.7469 - mse: 256.6910 - val_loss: 225.8520 - val_mse: 226.6465\n",
            "Epoch 196/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 255.5884 - mse: 255.4438 - val_loss: 224.6391 - val_mse: 225.4518\n",
            "Epoch 197/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 253.6902 - mse: 254.1711 - val_loss: 223.4206 - val_mse: 224.2516\n",
            "Epoch 198/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 251.6929 - mse: 252.9017 - val_loss: 222.1845 - val_mse: 223.0341\n",
            "Epoch 199/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 251.8159 - mse: 251.6134 - val_loss: 220.9515 - val_mse: 221.8197\n",
            "Epoch 200/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 249.9535 - mse: 250.3548 - val_loss: 219.7381 - val_mse: 220.6247\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f2a6e1e4860>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFu2k4J_spfi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "cf91ab58-7c85-4ffc-9990-117b2f3faee2"
      },
      "source": [
        "loss, mse = model.evaluate(test_ds)\n",
        "print(\"Mean Squared Error - Test Data\", mse)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4/4 [==============================] - 0s 4ms/step - loss: 268.8544 - mse: 264.8756\n",
            "Mean Squared Error - Test Data 264.87564\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}